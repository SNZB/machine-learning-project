{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN (1) part 2 done in 0.425881s\n",
      "EN part 2 done in 0.46306s\n",
      "SG(1) part 2 done in 1.285997s\n",
      "FR part 2 done in 0.159711s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#reading file\n",
    "\n",
    "def read_labeled_file(filename):\n",
    "    '''\n",
    "    Read an apropriate file.\n",
    "    \n",
    "    Takes the path to file\n",
    "    returns a list of (word, tag) tuples\n",
    "    '''\n",
    "    result = []\n",
    "    singletweet = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line == \"\\n\":\n",
    "                result.append(singletweet)\n",
    "                singletweet = []\n",
    "            else:\n",
    "                linelist = line.strip(\"\\n\").split(\" \")\n",
    "                singletweet.append(tuple(linelist))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def estimate_emission_param(data):\n",
    "    '''\n",
    "    Takes a list of (word, tag) tuple.\n",
    "    returns:\n",
    "        - iterable of all available words\n",
    "        - iterable of all available tag\n",
    "        - dictionary of emission parameter \n",
    "          with key <word, tag>\n",
    "    '''\n",
    "    tag_to_word_count = {}\n",
    "    word_count = {}\n",
    "    tag_count = {}\n",
    "    \n",
    "    for tweet in data: \n",
    "        for tagged_word in tweet:\n",
    "            # loops through the data and get respective counts\n",
    "            word = tagged_word[0]\n",
    "            tag = tagged_word[1]\n",
    "            \n",
    "            #incrementing counts\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "            tag_count[tag] = tag_count.get(tag, 0) + 1\n",
    "            tag_to_word_count[tagged_word] = tag_to_word_count\\\n",
    "                                              .get(tagged_word, 0) + 1\n",
    "                \n",
    "    # once count is settled, we can get emission parameter\n",
    "    emission_parameter = {k: tag_to_word_count[k]/tag_count[k[1]] \n",
    "                          for k in tag_to_word_count}\n",
    "    \n",
    "    return word_count.keys(), tag_count.keys(), emission_parameter\n",
    "def supress_infrequent_words(data, k=3):\n",
    "    '''\n",
    "    Takes a list of (word, tag) tuple\n",
    "    returns a new list with infrequent\n",
    "    words replaced with #UNK#\n",
    "    \n",
    "    k = number of occurence that is \n",
    "        considered to be known\n",
    "    '''\n",
    "    word_count = {}\n",
    "\n",
    "    #get word count\n",
    "    for tweet in data:\n",
    "        for tagged_word in tweet:\n",
    "            word = tagged_word[0]\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "                \n",
    "    #generate new list\n",
    "    result = []\n",
    "    newtweet = []\n",
    "    for tweet in data:\n",
    "        for tagged_word in tweet:\n",
    "            word = tagged_word[0]\n",
    "            if word_count[word] >= k:\n",
    "                newtweet.append(tagged_word)\n",
    "            else:\n",
    "                tag = tagged_word[1]\n",
    "                newtweet.append((\"#UNK#\",tag))\n",
    "        result.append(newtweet)\n",
    "        newtweet = []\n",
    "        \n",
    "    return result\n",
    "def single_sentiment_analysis(tags, param, word):\n",
    "    '''\n",
    "    Takes:\n",
    "        - a list of of discovered tags\n",
    "        - a dictionary for emission parameter\n",
    "        - the word to be tagged\n",
    "    return:\n",
    "        - a tuple of (word, predicted_tag)\n",
    "    '''\n",
    "    \n",
    "    mle = (word, \"O\") #assuming tag O for undiscovered word\n",
    "    mle_value = 0\n",
    "    for t in tags:\n",
    "        if (word, t) in param:\n",
    "            if param[(word, t)] > mle_value:\n",
    "                mle = (word, t)\n",
    "                mle_value = param[(word, t)]\n",
    "    return mle\n",
    "def write_simple_prediction(country, part, words, tags, param):\n",
    "    '''\n",
    "    takes:\n",
    "        - countri string (\"CN\",\"EN\" etc)\n",
    "        - part string (for question part 1, part 2 etc)\n",
    "        - a list of discovered words\n",
    "        - a list of discovered tags\n",
    "        - a dictionary of emission parameter\n",
    "    '''\n",
    "    input_filename = country + \"/dev.in\"\n",
    "    output_filename = country + \"/dev.p\"+part+\".out\"\n",
    "    with open(input_filename, \"r\") as inputfile:\n",
    "        with open(output_filename, \"w\") as outputfile:\n",
    "            for line in inputfile:\n",
    "                if line ==\"\\n\":\n",
    "                    outputfile.write(\"\\n\")\n",
    "                    continue\n",
    "                if line.strip(\"\\n\") in words:\n",
    "                    pred = single_sentiment_analysis(tags, param, line.strip(\"\\n\"))\n",
    "                    outputfile.write(\" \".join(pred)+\"\\n\")\n",
    "                else:\n",
    "                    outputfile.write(\"#UNK# O\\n\")\n",
    "\n",
    "from datetime import datetime\n",
    "# now we do it for all 4 countries\n",
    "# recording timing\n",
    "for c in [\"CN (1)\", \"EN\", \"SG(1)\", \"FR\"]:\n",
    "    start = datetime.now()\n",
    "    data = read_labeled_file(c+\"/train\")\n",
    "    sdata = supress_infrequent_words(data)\n",
    "    words, tags, em_param = estimate_emission_param(data)\n",
    "    write_simple_prediction(c,\"2\",\n",
    "                    words, tags, em_param)\n",
    "    end = datetime.now()\n",
    "    delt = end - start\n",
    "    print(\"{} part 2 done in {}.{}s\"\\\n",
    "          .format(c, delt.seconds, delt.microseconds))\n",
    "\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_parameter(data):\n",
    "    tag_transition_count={}\n",
    "    #(tag0,tag1):count\n",
    "    tag_count = {}\n",
    "    #tag0:count\n",
    "    for tweet in data:\n",
    "        for i in range(len(tweet)-1):\n",
    "            tag1 = tweet[i][1]\n",
    "            if i ==0:\n",
    "                tag0 = \"START\"\n",
    "            else:\n",
    "                tag0 = tweet[i-1][1]\n",
    "            tag_count[tag0]=tag_count.get(tag0,0)+1\n",
    "            tag_transition=(tag0,tag1)\n",
    "            tag_transition_count[tag_transition] = tag_transition_count.get(tag_transition,0)+1\n",
    "            if i == len(tweet)-1:\n",
    "                tag0 = tag[1]\n",
    "                tag[1] = \"STOP\"\n",
    "                tag_count[tag0]=tag_count.get(tag0,0)+1\n",
    "                tag_transition=(tag0,tag1)\n",
    "                tag_transition_count[tag_transition] = tag_transition_count.get(tag_transition,0)+1\n",
    "    transition_parameter = {k: tag_transition_count[k]/tag_count[k[0]]\n",
    "                        for k in tag_transition_count}\n",
    "    return transition_parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN (1) part 3 done in 1.282776s\n",
      "EN part 3 done in 0.338971s\n",
      "SG(1) part 3 done in 5.590018s\n",
      "FR part 3 done in 0.267118s\n"
     ]
    }
   ],
   "source": [
    "# takes in emmision para & transition para & file\n",
    "# predict tag seq\n",
    "# obs-words\n",
    "# states-tags\n",
    "#start-word/all start-waord ---start_p* word in tag/word in all tag-----emmision_p\n",
    "\n",
    "def create_start_parameter(data):\n",
    "    start_tag_count={}\n",
    "    total_tweet_count =0\n",
    "    #tag:count\n",
    "    for tweet in data:\n",
    "        start_tag = tweet[0][1]\n",
    "        start_tag_count[start_tag]=start_tag_count.get(start_tag,0)+1\n",
    "        total_tweet_count += 1\n",
    "    start_parameter = {k: start_tag_count[k]/total_tweet_count\n",
    "                        for k in start_tag_count}\n",
    "    return start_parameter\n",
    "\n",
    "    \n",
    "        \n",
    "def viterbi(words, tags, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    # Initialize base cases (t == 0)\n",
    "    for y in tags:\n",
    "        V[0][y] = start_p.get(y,0) * emit_p.get((words[0],y),0)\n",
    "        path[y] = [y]\n",
    "    # Run Viterbi for t > 0\n",
    "    for t in range(1,len(words)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "\n",
    "        for y in tags:\n",
    "            (prob, tag) = max([(V[t-1][y0] * trans_p.get((y0,y),0) * emit_p.get((words[t],y),0), y0) \n",
    "                               for y0 in tags])\n",
    "            V[t][y] = prob\n",
    "            newpath[y] = path[tag] + [y]\n",
    "\n",
    "        # Don't need to remember the old paths\n",
    "        path = newpath\n",
    "    (prob, tag) = max([(V[len(words) - 1][y], y) for y in tags])\n",
    "    return (prob, path[tag])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_hmm_prediction(country, part, prediction_function,\n",
    "                         word_sequence,tags, start_param, emit_param, trans_param):\n",
    "    '''\n",
    "    Function to write HMM prediction\n",
    "    '''\n",
    "    input_filename = country + \"/dev.in\"\n",
    "    output_filename = country + \"/dev.p\"+part+\".out\"\n",
    "    indata = []\n",
    "    #read and separate tweets\n",
    "    with open(input_filename, \"r\") as infile:\n",
    "        indata = infile.read().strip('\\n').split('\\n\\n') \n",
    "    \n",
    "    with open(output_filename, \"w\") as outfile:\n",
    "        for tweet in indata:\n",
    "            word_sequence = tweet.split('\\n')\n",
    "            predicted_tag_sequence = prediction_function(word_sequence,\n",
    "                                                tags, start_param, trans_param, emit_param)[1]\n",
    "            if len(word_sequence) != len(predicted_tag_sequence):\n",
    "                print(\"WARNING!! Different length {} / {}\"\\\n",
    "                      .format(word_sequence, predicted_tag_sequence))\n",
    "            for i in range(len(word_sequence)):\n",
    "                line = \"{} {}\\n\".format(word_sequence[i], \n",
    "                                        predicted_tag_sequence[i])\n",
    "                outfile.write(line)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "for c in [\"CN (1)\", \"EN\", \"SG(1)\", \"FR\"]:\n",
    "    start = datetime.now()\n",
    "    data = read_labeled_file(c+\"/train\")\n",
    "    supress_data = supress_infrequent_words(data)\n",
    "    words, tags, emit_param = estimate_emission_param(supress_data)\n",
    "    trans_param = create_transition_parameter(supress_data)\n",
    "    start_param = create_start_parameter(supress_data)\n",
    "    write_hmm_prediction(c,\"3\", viterbi,\n",
    "                        words, tags, start_param, emit_param, trans_param)\n",
    "    end = datetime.now()\n",
    "    delt = end - start\n",
    "    print(\"{} part 3 done in {}.{}s\"\\\n",
    "          .format(c, delt.seconds, delt.microseconds))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
